{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d79ea5-2843-4a5a-8290-b915b47a98d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:comtypes.client._code_cache:Imported existing <module 'comtypes.gen' from 'C:\\\\Users\\\\VATSAL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\comtypes\\\\gen\\\\__init__.py'>\n",
      "INFO:comtypes.client._code_cache:Using writeable comtypes cache directory: 'C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\comtypes\\gen'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 196, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 495, in _make_request\n",
      "    conn.request(\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 398, in request\n",
      "    self.endheaders()\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1331, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1091, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1035, in send\n",
      "    self.connect()\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 236, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 211, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001BE28958D10>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BE28958D10>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1431, in run\n",
      "    self.function(*self.args, **self.kwargs)\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flask_ngrok.py\", line 70, in start_ngrok\n",
      "    ngrok_address = _run_ngrok()\n",
      "                    ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flask_ngrok.py\", line 35, in _run_ngrok\n",
      "    tunnel_url = requests.get(localhost_url).text  # Get the tunnel information\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BE28958D10>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "INFO:__main__:User said: play music\n",
      "INFO:__main__:Processing command: play music\n",
      "INFO:__main__:Playing rock music.\n",
      "INFO:__main__:Response: Playing rock music.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:04:16] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: play music play music\n",
      "INFO:__main__:Processing command: play music play music\n",
      "INFO:__main__:Playing rock music.\n",
      "INFO:__main__:Response: Playing rock music.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:05:14] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: navigate to Berlin\n",
      "INFO:__main__:Processing command: navigate to Berlin\n",
      "INFO:__main__:Starting navigation to berlin.\n",
      "INFO:__main__:Response: Starting navigation to berlin.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:05:28] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: set temperature to 59\n",
      "INFO:__main__:Processing command: set temperature to 59\n",
      "INFO:__main__:Setting temperature to 59 degrees Celsius.\n",
      "INFO:__main__:Response: Setting temperature to 59 degrees Celsius.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:05:46] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template_string\n",
    "from flask_ngrok import run_with_ngrok\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pyttsx3\n",
    "import logging\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load models and tokenizers\n",
    "gpt2_model_name = \"gpt2-medium\"\n",
    "translation_model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "\n",
    "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "gpt2_model = transformers.AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "translator_tokenizer = AutoTokenizer.from_pretrained(translation_model_name)\n",
    "translator_model = AutoModelForSeq2SeqLM.from_pretrained(translation_model_name)\n",
    "\n",
    "# NLU pipeline for named entity recognition\n",
    "nlp = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Simulated car state\n",
    "car_state = {\n",
    "    \"temperature\": 22,\n",
    "    \"music_playing\": False,\n",
    "    \"navigation_active\": False,\n",
    "    \"windows\": \"closed\",\n",
    "    \"sunroof\": \"closed\",\n",
    "    \"location\": \"Unknown\",\n",
    "}\n",
    "\n",
    "# User profile for preferences\n",
    "user_profile = {\"name\": \"User\", \"preferences\": {\"music\": \"rock\", \"language\": \"en\"}}\n",
    "\n",
    "def generate_response(text):\n",
    "    input_ids = gpt2_tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    gpt2_model.config.pad_token_id = gpt2_tokenizer.eos_token_id\n",
    "    output = gpt2_model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=50,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50\n",
    "    )\n",
    "    response = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def process_command(text):\n",
    "    logger.info(f\"Processing command: {text}\")\n",
    "\n",
    "    text = text.lower()\n",
    "    if 'play music' in text:\n",
    "        return play_music()\n",
    "    elif 'navigate to' in text:\n",
    "        destination = re.sub(r'navigate to', '', text).strip()\n",
    "        return navigate_to(destination)\n",
    "    elif 'set temperature' in text:\n",
    "        temperature = re.search(r'\\d+', text)\n",
    "        if temperature:\n",
    "            return set_temperature(int(temperature.group()))\n",
    "    elif 'open' in text or 'close' in text:\n",
    "        return control_windows_or_sunroof(text)\n",
    "    elif 'sum' in text or 'calculate' in text:\n",
    "        return handle_math_expression(text)\n",
    "    else:\n",
    "        return \"I'm not sure how to help with that.\"\n",
    "\n",
    "def play_music():\n",
    "    global car_state\n",
    "    car_state[\"music_playing\"] = True\n",
    "    genre = user_profile['preferences']['music']\n",
    "    response = f\"Playing {genre} music.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def navigate_to(destination):\n",
    "    global car_state\n",
    "    car_state[\"navigation_active\"] = True\n",
    "    car_state[\"location\"] = destination\n",
    "    response = f\"Starting navigation to {destination}.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def set_temperature(temperature):\n",
    "    global car_state\n",
    "    car_state[\"temperature\"] = temperature\n",
    "    response = f\"Setting temperature to {temperature} degrees Celsius.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def control_windows_or_sunroof(text):\n",
    "    global car_state\n",
    "    if 'open' in text:\n",
    "        if 'window' in text:\n",
    "            car_state['windows'] = 'open'\n",
    "        if 'sunroof' in text:\n",
    "            car_state['sunroof'] = 'open'\n",
    "    elif 'close' in text:\n",
    "        if 'window' in text:\n",
    "            car_state['windows'] = 'closed'\n",
    "        if 'sunroof' in text:\n",
    "            car_state['sunroof'] = 'closed'\n",
    "    response = f\"Windows are {car_state['windows']} and sunroof is {car_state['sunroof']}.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def handle_math_expression(text):\n",
    "    try:\n",
    "        expression = re.sub(r'sum of|calculate', '', text).strip()\n",
    "        numbers = list(map(int, re.findall(r'\\d+', expression)))\n",
    "        result = sum(numbers)\n",
    "        response = f\"The result is {result}.\"\n",
    "        logger.info(response)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error handling math expression: {e}\")\n",
    "        return \"Error in calculating the result.\"\n",
    "\n",
    "# Flask app setup\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)  # Integrate Flask with ngrok\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template_string(\"\"\"\n",
    "    <!doctype html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>Car Voice Assistant</title>\n",
    "            <style>\n",
    "                body { font-family: Arial, sans-serif; }\n",
    "                .container { max-width: 600px; margin: auto; padding: 20px; text-align: center; }\n",
    "                .log { text-align: left; max-height: 200px; overflow-y: auto; background-color: #f1f1f1; padding: 10px; border: 1px solid #ccc; }\n",
    "                .hint { margin-top: 20px; font-style: italic; }\n",
    "            </style>\n",
    "            <script>\n",
    "                function startRecognition() {\n",
    "                    var recognition = new webkitSpeechRecognition();\n",
    "                    recognition.lang = \"en-US\";\n",
    "                    recognition.onresult = function(event) {\n",
    "                        document.getElementById('command').value = event.results[0][0].transcript;\n",
    "                        document.getElementById('commandForm').submit();\n",
    "                    }\n",
    "                    recognition.start();\n",
    "                }\n",
    "            </script>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>Test the Car Voice Assistant</h1>\n",
    "                <form id=\"commandForm\" action=\"/process\" method=\"post\">\n",
    "                    <input type=\"hidden\" id=\"command\" name=\"command\">\n",
    "                </form>\n",
    "                <button onclick=\"startRecognition()\">Press and Speak</button>\n",
    "                <div class=\"hint\">\n",
    "                    <p>Try saying:</p>\n",
    "                    <ul>\n",
    "                        <li>\"Play music\"</li>\n",
    "                        <li>\"Navigate to Berlin\"</li>\n",
    "                        <li>\"Set temperature to 24\"</li>\n",
    "                        <li>\"Open the windows\"</li>\n",
    "                        <li>\"Sum of 5 and 10\"</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "                <p>Response: {{response}}</p>\n",
    "                <h2>Background Processes</h2>\n",
    "                <div class=\"log\">{{log}}</div>\n",
    "                <h2>Technologies Used</h2>\n",
    "                <p>This application demonstrates the use of AI and NLP technologies such as:</p>\n",
    "                <ul>\n",
    "                    <li>Speech Recognition using Web Speech API</li>\n",
    "                    <li>Text Processing using GPT-2 and Transformers</li>\n",
    "                    <li>Natural Language Understanding using Named Entity Recognition</li>\n",
    "                    <li>Text-to-Speech using pyttsx3</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\", response=\"\", log=\"\")\n",
    "\n",
    "@app.route('/process', methods=['POST'])\n",
    "def process():\n",
    "    command = request.form['command']\n",
    "    logger.info(f\"User said: {command}\")\n",
    "    response = process_command(command)\n",
    "    logger.info(f\"Response: {response}\")\n",
    "\n",
    "    with open(\"log.txt\", \"a\") as log_file:\n",
    "        log_file.write(f\"User said: {command}\\n\")\n",
    "        log_file.write(f\"Response: {response}\\n\")\n",
    "\n",
    "    log_content = \"\"\n",
    "    with open(\"log.txt\", \"r\") as log_file:\n",
    "        log_content = log_file.read().replace(\"\\n\", \"<br>\")\n",
    "\n",
    "    return render_template_string(\"\"\"\n",
    "    <!doctype html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>Car Voice Assistant</title>\n",
    "            <style>\n",
    "                body { font-family: Arial, sans-serif; }\n",
    "                .container { max-width: 600px; margin: auto; padding: 20px; text-align: center; }\n",
    "                .log { text-align: left; max-height: 200px; overflow-y: auto; background-color: #f1f1f1; padding: 10px; border: 1px solid #ccc; }\n",
    "                .hint { margin-top: 20px; font-style: italic; }\n",
    "            </style>\n",
    "            <script>\n",
    "                function startRecognition() {\n",
    "                    var recognition = new webkitSpeechRecognition();\n",
    "                    recognition.lang = \"en-US\";\n",
    "                    recognition.onresult = function(event) {\n",
    "                        document.getElementById('command').value = event.results[0][0].transcript;\n",
    "                        document.getElementById('commandForm').submit();\n",
    "                    }\n",
    "                    recognition.start();\n",
    "                }\n",
    "            </script>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>Test the Car Voice Assistant</h1>\n",
    "                <form id=\"commandForm\" action=\"/process\" method=\"post\">\n",
    "                    <input type=\"hidden\" id=\"command\" name=\"command\">\n",
    "                </form>\n",
    "                <button onclick=\"startRecognition()\">Press and Speak</button>\n",
    "                <div class=\"hint\">\n",
    "                    <p>Try saying:</p>\n",
    "                    <ul>\n",
    "                        <li>\"Play music\"</li>\n",
    "                        <li>\"Navigate to Berlin\"</li>\n",
    "                        <li>\"Set temperature to 24\"</li>\n",
    "                        <li>\"Open the windows\"</li>\n",
    "                        <li>\"Sum of 5 and 10\"</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "                <p>Response: {{response}}</p>\n",
    "                <h2>Background Processes</h2>\n",
    "                <div class=\"log\">{{log}}</div>\n",
    "                <h2>Technologies Used</h2>\n",
    "                <p>This application demonstrates the use of AI and NLP technologies such as:</p>\n",
    "                <ul>\n",
    "                    <li>Speech Recognition using Web Speech API</li>\n",
    "                    <li>Text Processing using GPT-2 and Transformers</li>\n",
    "                    <li>Natural Language Understanding using Named Entity Recognition</li>\n",
    "                    <li>Text-to-Speech using pyttsx3</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\", response=response, log=log_content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a9415f-c026-4898-9ee6-1560baf4c3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:comtypes.client._code_cache:Imported existing <module 'comtypes.gen' from 'C:\\\\Users\\\\VATSAL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\comtypes\\\\gen\\\\__init__.py'>\n",
      "INFO:comtypes.client._code_cache:Using writeable comtypes cache directory: 'C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\comtypes\\gen'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:__main__:User said: set temperature to 59\n",
      "INFO:__main__:Processing command: set temperature to 59\n",
      "INFO:__main__:Setting temperature to 59 degrees Celsius.\n",
      "INFO:__main__:Response: Setting temperature to 59 degrees Celsius.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:07:45] \"POST /process HTTP/1.1\" 200 -\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 196, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 495, in _make_request\n",
      "    conn.request(\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 398, in request\n",
      "    self.endheaders()\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1331, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1091, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1035, in send\n",
      "    self.connect()\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 236, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 211, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x00000201FA4D64B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000201FA4D64B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1431, in run\n",
      "    self.function(*self.args, **self.kwargs)\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flask_ngrok.py\", line 70, in start_ngrok\n",
      "    ngrok_address = _run_ngrok()\n",
      "                    ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flask_ngrok.py\", line 35, in _run_ngrok\n",
      "    tunnel_url = requests.get(localhost_url).text  # Get the tunnel information\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000201FA4D64B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "INFO:__main__:User said: play music\n",
      "INFO:__main__:Processing command: play music\n",
      "INFO:__main__:Playing rock music.\n",
      "INFO:__main__:Response: Playing rock music.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:08:10] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: navigate to America\n",
      "INFO:__main__:Processing command: navigate to America\n",
      "INFO:__main__:Starting navigation to america.\n",
      "INFO:__main__:Response: Starting navigation to america.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:08:24] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: open sunroof\n",
      "INFO:__main__:Processing command: open sunroof\n",
      "INFO:__main__:Windows are closed and sunroof is open.\n",
      "INFO:__main__:Response: Windows are closed and sunroof is open.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:08:36] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: window open window\n",
      "INFO:__main__:Processing command: window open window\n",
      "INFO:__main__:Windows are open and sunroof is open.\n",
      "INFO:__main__:Response: Windows are open and sunroof is open.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:08:51] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: close the window\n",
      "INFO:__main__:Processing command: close the window\n",
      "INFO:__main__:Windows are closed and sunroof is open.\n",
      "INFO:__main__:Response: Windows are closed and sunroof is open.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:09:13] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: close the sunroof and open the window\n",
      "INFO:__main__:Processing command: close the sunroof and open the window\n",
      "INFO:__main__:Windows are open and sunroof is open.\n",
      "INFO:__main__:Response: Windows are open and sunroof is open.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:09:22] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: close\n",
      "INFO:__main__:Processing command: close\n",
      "INFO:__main__:Windows are open and sunroof is open.\n",
      "INFO:__main__:Response: Windows are open and sunroof is open.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:09:40] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: Close sunroof\n",
      "INFO:__main__:Processing command: Close sunroof\n",
      "INFO:__main__:Windows are open and sunroof is closed.\n",
      "INFO:__main__:Response: Windows are open and sunroof is closed.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:09:48] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: music\n",
      "INFO:__main__:Processing command: music\n",
      "INFO:__main__:Response: I'm not sure how to help with that.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:09:57] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: turn off music\n",
      "INFO:__main__:Processing command: turn off music\n",
      "INFO:__main__:Response: I'm not sure how to help with that.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:10:03] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: what's the temperature to 99\n",
      "INFO:__main__:Processing command: what's the temperature to 99\n",
      "INFO:__main__:Response: I'm not sure how to help with that.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:10:16] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: set temperature to 99\n",
      "INFO:__main__:Processing command: set temperature to 99\n",
      "INFO:__main__:Setting temperature to 99 degrees Celsius.\n",
      "INFO:__main__:Response: Setting temperature to 99 degrees Celsius.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:10:26] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: play some music\n",
      "INFO:__main__:Processing command: play some music\n",
      "INFO:__main__:Response: I'm not sure how to help with that.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:10:32] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: play some music\n",
      "INFO:__main__:Processing command: play some music\n",
      "INFO:__main__:Response: I'm not sure how to help with that.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:10:57] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: play some music\n",
      "INFO:__main__:Processing command: play some music\n",
      "INFO:__main__:Response: I'm not sure how to help with that.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:11:12] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:11:30] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: play music\n",
      "INFO:__main__:Processing command: play music\n",
      "INFO:__main__:Playing rock music.\n",
      "INFO:__main__:Response: Playing rock music.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:11:41] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template_string\n",
    "from flask_ngrok import run_with_ngrok\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pyttsx3\n",
    "import logging\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load models and tokenizers\n",
    "gpt2_model_name = \"gpt2-medium\"\n",
    "translation_model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "\n",
    "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "gpt2_model = transformers.AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "translator_tokenizer = AutoTokenizer.from_pretrained(translation_model_name)\n",
    "translator_model = AutoModelForSeq2SeqLM.from_pretrained(translation_model_name)\n",
    "\n",
    "# NLU pipeline for named entity recognition\n",
    "nlp = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Simulated car state\n",
    "car_state = {\n",
    "    \"temperature\": 22,\n",
    "    \"music_playing\": False,\n",
    "    \"navigation_active\": False,\n",
    "    \"windows\": \"closed\",\n",
    "    \"sunroof\": \"closed\",\n",
    "    \"location\": \"Unknown\",\n",
    "}\n",
    "\n",
    "# User profile for preferences\n",
    "user_profile = {\"name\": \"User\", \"preferences\": {\"music\": \"rock\", \"language\": \"en\"}}\n",
    "\n",
    "def generate_response(text):\n",
    "    input_ids = gpt2_tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    gpt2_model.config.pad_token_id = gpt2_tokenizer.eos_token_id\n",
    "    output = gpt2_model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=50,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50\n",
    "    )\n",
    "    response = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def process_command(text):\n",
    "    logger.info(f\"Processing command: {text}\")\n",
    "\n",
    "    text = text.lower()\n",
    "    if 'play music' in text:\n",
    "        return play_music()\n",
    "    elif 'navigate to' in text:\n",
    "        destination = re.sub(r'navigate to', '', text).strip()\n",
    "        return navigate_to(destination)\n",
    "    elif 'set temperature' in text:\n",
    "        temperature = re.search(r'\\d+', text)\n",
    "        if temperature:\n",
    "            return set_temperature(int(temperature.group()))\n",
    "    elif 'open' in text or 'close' in text:\n",
    "        return control_windows_or_sunroof(text)\n",
    "    elif 'sum' in text or 'calculate' in text:\n",
    "        return handle_math_expression(text)\n",
    "    else:\n",
    "        return \"I'm not sure how to help with that.\"\n",
    "\n",
    "def play_music():\n",
    "    global car_state\n",
    "    car_state[\"music_playing\"] = True\n",
    "    genre = user_profile['preferences']['music']\n",
    "    response = f\"Playing {genre} music.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def navigate_to(destination):\n",
    "    global car_state\n",
    "    car_state[\"navigation_active\"] = True\n",
    "    car_state[\"location\"] = destination\n",
    "    response = f\"Starting navigation to {destination}.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def set_temperature(temperature):\n",
    "    global car_state\n",
    "    car_state[\"temperature\"] = temperature\n",
    "    response = f\"Setting temperature to {temperature} degrees Celsius.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def control_windows_or_sunroof(text):\n",
    "    global car_state\n",
    "    if 'open' in text:\n",
    "        if 'window' in text:\n",
    "            car_state['windows'] = 'open'\n",
    "        if 'sunroof' in text:\n",
    "            car_state['sunroof'] = 'open'\n",
    "    elif 'close' in text:\n",
    "        if 'window' in text:\n",
    "            car_state['windows'] = 'closed'\n",
    "        if 'sunroof' in text:\n",
    "            car_state['sunroof'] = 'closed'\n",
    "    response = f\"Windows are {car_state['windows']} and sunroof is {car_state['sunroof']}.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def handle_math_expression(text):\n",
    "    try:\n",
    "        expression = re.sub(r'sum of|calculate', '', text).strip()\n",
    "        numbers = list(map(int, re.findall(r'\\d+', expression)))\n",
    "        result = sum(numbers)\n",
    "        response = f\"The result is {result}.\"\n",
    "        logger.info(response)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error handling math expression: {e}\")\n",
    "        return \"Error in calculating the result.\"\n",
    "\n",
    "# Flask app setup\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)  # Integrate Flask with ngrok\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template_string(\"\"\"\n",
    "    <!doctype html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>Car Voice Assistant</title>\n",
    "            <script>\n",
    "                function startRecognition() {\n",
    "                    var recognition = new webkitSpeechRecognition();\n",
    "                    recognition.lang = \"en-US\";\n",
    "                    recognition.onresult = function(event) {\n",
    "                        document.getElementById('command').value = event.results[0][0].transcript;\n",
    "                        document.getElementById('commandForm').submit();\n",
    "                    }\n",
    "                    recognition.start();\n",
    "                }\n",
    "            </script>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Test the Car Voice Assistant</h1>\n",
    "            <p>Press the button and speak a command. Example commands:</p>\n",
    "            <ul>\n",
    "                <li>\"Play music\"</li>\n",
    "                <li>\"Navigate to Berlin\"</li>\n",
    "                <li>\"Set temperature to 24 degrees\"</li>\n",
    "                <li>\"Open the windows\"</li>\n",
    "                <li>\"Calculate the sum of 5 and 10\"</li>\n",
    "            </ul>\n",
    "            <form id=\"commandForm\" action=\"/process\" method=\"post\">\n",
    "                <input type=\"hidden\" id=\"command\" name=\"command\">\n",
    "            </form>\n",
    "            <button onclick=\"startRecognition()\">Press and Speak</button>\n",
    "            <p>{{response}}</p>\n",
    "            <h2>Background Process</h2>\n",
    "            <pre>{{background}}</pre>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\", response=\"\", background=\"Waiting for command...\")\n",
    "\n",
    "@app.route('/process', methods=['POST'])\n",
    "def process():\n",
    "    command = request.form['command']\n",
    "    logger.info(f\"User said: {command}\")\n",
    "    response = process_command(command)\n",
    "    background = f\"Processed command: {command}\\nResponse: {response}\\n\\nCurrent Car State: {car_state}\"\n",
    "    logger.info(f\"Response: {response}\")\n",
    "    return render_template_string(\"\"\"\n",
    "    <!doctype html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>Car Voice Assistant</title>\n",
    "            <script>\n",
    "                function startRecognition() {\n",
    "                    var recognition = new webkitSpeechRecognition();\n",
    "                    recognition.lang = \"en-US\";\n",
    "                    recognition.onresult = function(event) {\n",
    "                        document.getElementById('command').value = event.results[0][0].transcript;\n",
    "                        document.getElementById('commandForm').submit();\n",
    "                    }\n",
    "                    recognition.start();\n",
    "                }\n",
    "            </script>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Test the Car Voice Assistant</h1>\n",
    "            <p>Press the button and speak a command. Example commands:</p>\n",
    "            <ul>\n",
    "                <li>\"Play music\"</li>\n",
    "                <li>\"Navigate to Berlin\"</li>\n",
    "                <li>\"Set temperature to 24 degrees\"</li>\n",
    "                <li>\"Open the windows\"</li>\n",
    "                <li>\"Calculate the sum of 5 and 10\"</li>\n",
    "            </ul>\n",
    "            <form id=\"commandForm\" action=\"/process\" method=\"post\">\n",
    "                <input type=\"hidden\" id=\"command\" name=\"command\">\n",
    "            </form>\n",
    "            <button onclick=\"startRecognition()\">Press and Speak</button>\n",
    "            <p>Response: {{response}}</p>\n",
    "            <h2>Background Process</h2>\n",
    "            <pre>{{background}}</pre>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\", response=response, background=background)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165ed05-b4cf-447f-a6c5-9f930eae4894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "INFO:comtypes.client._code_cache:Imported existing <module 'comtypes.gen' from 'C:\\\\Users\\\\VATSAL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\comtypes\\\\gen\\\\__init__.py'>\n",
      "INFO:comtypes.client._code_cache:Using writeable comtypes cache directory: 'C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\comtypes\\gen'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 196, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 495, in _make_request\n",
      "    conn.request(\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 398, in request\n",
      "    self.endheaders()\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1331, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1091, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1035, in send\n",
      "    self.connect()\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 236, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py\", line 211, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000015AD1F84AA0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000015AD1F84AA0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1431, in run\n",
      "    self.function(*self.args, **self.kwargs)\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flask_ngrok.py\", line 70, in start_ngrok\n",
      "    ngrok_address = _run_ngrok()\n",
      "                    ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flask_ngrok.py\", line 35, in _run_ngrok\n",
      "    tunnel_url = requests.get(localhost_url).text  # Get the tunnel information\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VATSAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000015AD1F84AA0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "INFO:__main__:User said: set temperature to 94 degrees\n",
      "INFO:__main__:Processing command: set temperature to 94 degrees\n",
      "INFO:__main__:Setting temperature to 94 degrees Celsius.\n",
      "INFO:__main__:Response: Setting temperature to 94 degrees Celsius.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:19:54] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:20:10] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: play music\n",
      "INFO:__main__:Processing command: play music\n",
      "INFO:__main__:Playing rock music.\n",
      "INFO:__main__:Response: Playing rock music.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:20:20] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: set temperature to 97\n",
      "INFO:__main__:Processing command: set temperature to 97\n",
      "INFO:__main__:Setting temperature to 97 degrees Celsius.\n",
      "INFO:__main__:Response: Setting temperature to 97 degrees Celsius.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:20:26] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: navigate to\n",
      "INFO:__main__:Processing command: navigate to\n",
      "INFO:__main__:Starting navigation to .\n",
      "INFO:__main__:Response: Starting navigation to .\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:20:33] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: navigation to Surat\n",
      "INFO:__main__:Processing command: navigation to Surat\n",
      "INFO:__main__:Response: I'm not sure how to help with that.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:20:40] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: navigate to Surat\n",
      "INFO:__main__:Processing command: navigate to Surat\n",
      "INFO:__main__:Starting navigation to surat.\n",
      "INFO:__main__:Response: Starting navigation to surat.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:20:49] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: skip to Japan\n",
      "INFO:__main__:Processing command: skip to Japan\n",
      "INFO:__main__:Response: I'm not sure how to help with that.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:21:00] \"POST /process HTTP/1.1\" 200 -\n",
      "INFO:__main__:User said: navigate to Japan\n",
      "INFO:__main__:Processing command: navigate to Japan\n",
      "INFO:__main__:Starting navigation to japan.\n",
      "INFO:__main__:Response: Starting navigation to japan.\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Aug/2024 01:21:08] \"POST /process HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template_string\n",
    "from flask_ngrok import run_with_ngrok\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pyttsx3\n",
    "import logging\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load models and tokenizers\n",
    "gpt2_model_name = \"gpt2-medium\"\n",
    "translation_model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "\n",
    "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "gpt2_model = transformers.AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "translator_tokenizer = AutoTokenizer.from_pretrained(translation_model_name)\n",
    "translator_model = AutoModelForSeq2SeqLM.from_pretrained(translation_model_name)\n",
    "\n",
    "# NLU pipeline for named entity recognition\n",
    "nlp = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Simulated car state\n",
    "car_state = {\n",
    "    \"temperature\": 22,\n",
    "    \"music_playing\": False,\n",
    "    \"navigation_active\": False,\n",
    "    \"windows\": \"closed\",\n",
    "    \"sunroof\": \"closed\",\n",
    "    \"location\": \"Unknown\",\n",
    "}\n",
    "\n",
    "# User profile for preferences\n",
    "user_profile = {\"name\": \"User\", \"preferences\": {\"music\": \"rock\", \"language\": \"en\"}}\n",
    "\n",
    "def generate_response(text):\n",
    "    input_ids = gpt2_tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    gpt2_model.config.pad_token_id = gpt2_tokenizer.eos_token_id\n",
    "    output = gpt2_model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=50,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50\n",
    "    )\n",
    "    response = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def process_command(text):\n",
    "    logger.info(f\"Processing command: {text}\")\n",
    "\n",
    "    text = text.lower()\n",
    "    if 'play music' in text:\n",
    "        return play_music()\n",
    "    elif 'navigate to' in text:\n",
    "        destination = re.sub(r'navigate to', '', text).strip()\n",
    "        return navigate_to(destination)\n",
    "    elif 'set temperature' in text:\n",
    "        temperature = re.search(r'\\d+', text)\n",
    "        if temperature:\n",
    "            return set_temperature(int(temperature.group()))\n",
    "    elif 'open' in text or 'close' in text:\n",
    "        return control_windows_or_sunroof(text)\n",
    "    else:\n",
    "        return \"I'm not sure how to help with that.\"\n",
    "\n",
    "def play_music():\n",
    "    global car_state\n",
    "    car_state[\"music_playing\"] = True\n",
    "    genre = user_profile['preferences']['music']\n",
    "    response = f\"Playing {genre} music.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def navigate_to(destination):\n",
    "    global car_state\n",
    "    car_state[\"navigation_active\"] = True\n",
    "    car_state[\"location\"] = destination\n",
    "    response = f\"Starting navigation to {destination}.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def set_temperature(temperature):\n",
    "    global car_state\n",
    "    car_state[\"temperature\"] = temperature\n",
    "    response = f\"Setting temperature to {temperature} degrees Celsius.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "def control_windows_or_sunroof(text):\n",
    "    global car_state\n",
    "    if 'open' in text:\n",
    "        if 'window' in text:\n",
    "            car_state['windows'] = 'open'\n",
    "        if 'sunroof' in text:\n",
    "            car_state['sunroof'] = 'open'\n",
    "    elif 'close' in text:\n",
    "        if 'window' in text:\n",
    "            car_state['windows'] = 'closed'\n",
    "        if 'sunroof' in text:\n",
    "            car_state['sunroof'] = 'closed'\n",
    "    response = f\"Windows are {car_state['windows']} and sunroof is {car_state['sunroof']}.\"\n",
    "    logger.info(response)\n",
    "    return response\n",
    "\n",
    "# Flask app setup\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)  # Integrate Flask with ngrok\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template_string(\"\"\"\n",
    "    <!doctype html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>Car Voice Assistant</title>\n",
    "            <script>\n",
    "                function startRecognition() {\n",
    "                    var recognition = new webkitSpeechRecognition();\n",
    "                    recognition.lang = \"en-US\";\n",
    "                    recognition.onresult = function(event) {\n",
    "                        document.getElementById('command').value = event.results[0][0].transcript;\n",
    "                        document.getElementById('commandForm').submit();\n",
    "                    }\n",
    "                    recognition.start();\n",
    "                }\n",
    "                function speak(text) {\n",
    "                    var synth = window.speechSynthesis;\n",
    "                    var utterance = new SpeechSynthesisUtterance(text);\n",
    "                    synth.speak(utterance);\n",
    "                }\n",
    "            </script>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Test the Car Voice Assistant</h1>\n",
    "            <p>Press the button and speak a command. Example commands:</p>\n",
    "            <ul>\n",
    "                <li>\"Play music\"</li>\n",
    "                <li>\"Navigate to Berlin\"</li>\n",
    "                <li>\"Set temperature to 24 degrees\"</li>\n",
    "                <li>\"Open the windows\"</li>\n",
    "            </ul>\n",
    "            <form id=\"commandForm\" action=\"/process\" method=\"post\">\n",
    "                <input type=\"hidden\" id=\"command\" name=\"command\">\n",
    "            </form>\n",
    "            <button onclick=\"startRecognition()\">Press and Speak</button>\n",
    "            <p id=\"response\">{{response}}</p>\n",
    "            <h2>Background Process</h2>\n",
    "            <pre>{{background}}</pre>\n",
    "            <script>\n",
    "                var response = \"{{response}}\";\n",
    "                if (response) {\n",
    "                    speak(response);\n",
    "                }\n",
    "            </script>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\", response=\"\", background=\"Waiting for command...\")\n",
    "\n",
    "@app.route('/process', methods=['POST'])\n",
    "def process():\n",
    "    command = request.form['command']\n",
    "    logger.info(f\"User said: {command}\")\n",
    "    response = process_command(command)\n",
    "    background = f\"Processed command: {command}\\nResponse: {response}\\n\\nCurrent Car State: {car_state}\"\n",
    "    logger.info(f\"Response: {response}\")\n",
    "    return render_template_string(\"\"\"\n",
    "    <!doctype html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>Car Voice Assistant</title>\n",
    "            <script>\n",
    "                function startRecognition() {\n",
    "                    var recognition = new webkitSpeechRecognition();\n",
    "                    recognition.lang = \"en-US\";\n",
    "                    recognition.onresult = function(event) {\n",
    "                        document.getElementById('command').value = event.results[0][0].transcript;\n",
    "                        document.getElementById('commandForm').submit();\n",
    "                    }\n",
    "                    recognition.start();\n",
    "                }\n",
    "                function speak(text) {\n",
    "                    var synth = window.speechSynthesis;\n",
    "                    var utterance = new SpeechSynthesisUtterance(text);\n",
    "                    synth.speak(utterance);\n",
    "                }\n",
    "            </script>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Test the Car Voice Assistant</h1>\n",
    "            <p>Press the button and speak a command. Example commands:</p>\n",
    "            <ul>\n",
    "                <li>\"Play music\"</li>\n",
    "                <li>\"Navigate to Berlin\"</li>\n",
    "                <li>\"Set temperature to 24 degrees\"</li>\n",
    "                <li>\"Open the windows\"</li>\n",
    "            </ul>\n",
    "            <form id=\"commandForm\" action=\"/process\" method=\"post\">\n",
    "                <input type=\"hidden\" id=\"command\" name=\"command\">\n",
    "            </form>\n",
    "            <button onclick=\"startRecognition()\">Press and Speak</button>\n",
    "            <p id=\"response\">Response: {{response}}</p>\n",
    "            <h2>Background Process</h2>\n",
    "            <pre>{{background}}</pre>\n",
    "            <script>\n",
    "                var response = \"{{response}}\";\n",
    "                if (response) {\n",
    "                    speak(response);\n",
    "                }\n",
    "            </script>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\", response=response, background=background)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad3835-a3a1-46dc-abd2-ab8f47d171a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
